{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef71bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/anaconda3/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->kagglehub) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79b9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kangminji/.cache/kagglehub/datasets/aashita/nyt-comments/versions/13\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcb8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "article_lists = glob(path+'/*.*',recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1cdf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>abstract</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "      <th>articleWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58927e0495d0e0392607e1b3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By KEN BELSON</td>\n",
       "      <td>article</td>\n",
       "      <td>N.F.L. vs. Politics Has Been Battle All Season...</td>\n",
       "      <td>['Football', 'Super Bowl', 'National Football ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "      <td>12</td>\n",
       "      <td>2017-02-02 00:26:16</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>Despite the national tumult over immigration s...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/02/01/sports/supe...</td>\n",
       "      <td>1129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5893033d95d0e0392607e2d6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By UNKNOWN</td>\n",
       "      <td>article</td>\n",
       "      <td>Voice. Vice. Veracity.</td>\n",
       "      <td>['Television', 'Home Box Office', 'Girls (TV P...</td>\n",
       "      <td>1</td>\n",
       "      <td>Arts&amp;Leisure</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-02 10:00:24</td>\n",
       "      <td>Television</td>\n",
       "      <td>Our critics look at the impact of the HBO show...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/02/02/arts/televi...</td>\n",
       "      <td>3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5893039595d0e0392607e2da</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By MANOHLA DARGIS</td>\n",
       "      <td>article</td>\n",
       "      <td>A Stand-Up’s Downward Slide</td>\n",
       "      <td>['Movies', 'The Comedian (Movie)', 'De Niro, R...</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-02-02 10:01:53</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Joined by a cast that includes Edie Falco and ...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/02/02/movies/the-...</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5893109995d0e0392607e2ef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By ALEXANDRA S. LEVINE</td>\n",
       "      <td>article</td>\n",
       "      <td>New York Today: A Groundhog Has Her Day</td>\n",
       "      <td>['New York City', 'Groundhogs']</td>\n",
       "      <td>1</td>\n",
       "      <td>Metro</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-02 10:57:25</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Thursday: A meet-and-greet with Staten Island ...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>briefing</td>\n",
       "      <td>https://www.nytimes.com/2017/02/02/nyregion/ne...</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5893114495d0e0392607e2f1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>By BONNIE TSUI</td>\n",
       "      <td>article</td>\n",
       "      <td>A Swimmer’s Communion With the Ocean</td>\n",
       "      <td>['Travel and Vacations', 'Swimming', 'Oceans a...</td>\n",
       "      <td>1</td>\n",
       "      <td>Travel</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-02-02 11:00:03</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>“We swam in that heaving body of aquamarine, a...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/02/02/travel/hawa...</td>\n",
       "      <td>1283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID abstract                  byline documentType  \\\n",
       "0  58927e0495d0e0392607e1b3      NaN           By KEN BELSON      article   \n",
       "1  5893033d95d0e0392607e2d6      NaN              By UNKNOWN      article   \n",
       "2  5893039595d0e0392607e2da      NaN       By MANOHLA DARGIS      article   \n",
       "3  5893109995d0e0392607e2ef      NaN  By ALEXANDRA S. LEVINE      article   \n",
       "4  5893114495d0e0392607e2f1      NaN          By BONNIE TSUI      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  N.F.L. vs. Politics Has Been Battle All Season...   \n",
       "1                             Voice. Vice. Veracity.   \n",
       "2                        A Stand-Up’s Downward Slide   \n",
       "3            New York Today: A Groundhog Has Her Day   \n",
       "4               A Swimmer’s Communion With the Ocean   \n",
       "\n",
       "                                            keywords  multimedia  \\\n",
       "0  ['Football', 'Super Bowl', 'National Football ...           1   \n",
       "1  ['Television', 'Home Box Office', 'Girls (TV P...           1   \n",
       "2  ['Movies', 'The Comedian (Movie)', 'De Niro, R...           1   \n",
       "3                    ['New York City', 'Groundhogs']           1   \n",
       "4  ['Travel and Vacations', 'Swimming', 'Oceans a...           1   \n",
       "\n",
       "        newDesk  printPage              pubDate   sectionName  \\\n",
       "0        Sports         12  2017-02-02 00:26:16  Pro Football   \n",
       "1  Arts&Leisure          1  2017-02-02 10:00:24    Television   \n",
       "2       Weekend          5  2017-02-02 10:01:53       Unknown   \n",
       "3         Metro          0  2017-02-02 10:57:25       Unknown   \n",
       "4        Travel          4  2017-02-02 11:00:03       Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  Despite the national tumult over immigration s...  The New York Times   \n",
       "1  Our critics look at the impact of the HBO show...  The New York Times   \n",
       "2  Joined by a cast that includes Edie Falco and ...  The New York Times   \n",
       "3  Thursday: A meet-and-greet with Staten Island ...  The New York Times   \n",
       "4  “We swam in that heaving body of aquamarine, a...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \\\n",
       "0           News  https://www.nytimes.com/2017/02/01/sports/supe...   \n",
       "1           News  https://www.nytimes.com/2017/02/02/arts/televi...   \n",
       "2         Review  https://www.nytimes.com/2017/02/02/movies/the-...   \n",
       "3       briefing  https://www.nytimes.com/2017/02/02/nyregion/ne...   \n",
       "4           News  https://www.nytimes.com/2017/02/02/travel/hawa...   \n",
       "\n",
       "   articleWordCount  \n",
       "0              1129  \n",
       "1              3082  \n",
       "2               693  \n",
       "3              1049  \n",
       "4              1283  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(article_lists[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4b800",
   "metadata": {},
   "source": [
    "##### LSTM\n",
    "- 입력 : \"나는 파이썬을 좋아합니다. 따라서 나는 ___ 을 잘합니다.\"\n",
    "- 일반신경망 : 공부  ( 파이썬 정보가 희석... 잊어)\n",
    "- LSTM : 프로그래밍(오래된 정보도 기억)\n",
    "\n",
    "- 핵심 키워드\n",
    "    - 장기기억 : 중요한 정보는 오래기억\n",
    "    - 단기기억 : 불필요한 정보는 버림\n",
    "    - 순서이해 : 시간순서 이해\n",
    "\n",
    "##### 3개의 Gate를 통해 정보의 흐름을 제어\n",
    " - LSTM 셀 (한 시점 t)\n",
    "    - 입력 : $x_t$ (현재데이터)\n",
    "    - 이전 은닉상태 : $h_{t-1}$\n",
    "    - 이전 셀 상태 : $c_{t-1}$\n",
    "\n",
    "    --> forget gate --> input date   --> output gate\n",
    "        잊을데이터        추가할 데이터     출력할 데이터\n",
    "    \n",
    "    출력 $h_t$ ,  $c_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702adbf",
   "metadata": {},
   "source": [
    "Forget Gate(잊음 관문)\n",
    "\n",
    "\n",
    "$f_t$ = $s(w_f . [h_{t-1}, x_t ] + b_f )$\n",
    "\n",
    "s : sigmoid함수(0~1)\n",
    "\n",
    "이전 셀상태 : [1.5, -0.3, 2.1]\n",
    "현재입력 : '새로운 문장 입력'\n",
    "\n",
    "$f_t$ = [0.1,0.05,0.9]\n",
    "\n",
    "결과 : [ 1.5*0.1, -0.3*0.05, 2.1*0.9  ]  =  [0.15, -0.015, 1.89]\n",
    "\n",
    "첫 2개는 버리고 3번째는 유지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937c4b4",
   "metadata": {},
   "source": [
    "##### input gate : 입력\n",
    "- 첫 번째는 70%받고 두번째는 30% 받음\n",
    "\n",
    "##### Cell State 업데이트\n",
    " - 이전기억에서 필요한 것만 유지하고 새로운 정보에서 필요한 것만 유지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84d41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455d6281",
   "metadata": {},
   "source": [
    "input 게이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4e0a9",
   "metadata": {},
   "source": [
    "시점 t에서:\n",
    "- $x_t$      : 현재 입력 데이터 (벡터)\n",
    "- $h_{t-1}$  : 이전 시점의 은닉 상태 (벡터)\n",
    "- $C_{t-1}$  : 이전 시점의 셀 상태 (벡터) ← 장기 기억!\n",
    "- $W_*$, $U_*$ : 가중치 행렬\n",
    "- $b_*$      : 편향 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa5e5b",
   "metadata": {},
   "source": [
    "- $x_t$ = [0.2, -0.5, 0.8]      (3개 입력 피처)\n",
    "- $h_{t-1}$ = [0.1, 0.3, -0.2, 0.5]  (4개 은닉)\n",
    "- $C_{t-1}$ = [0.4, -0.1, 0.6, 0.2]  (4개 셀 상태)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009bbb3",
   "metadata": {},
   "source": [
    "Forget Gate ( $f_t$ ) - 어제 기억을 얼마나 유지할까\n",
    "- $f_t$ = $σ( W_f · [h_{t-1}, x_t] + b_f )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7bac5",
   "metadata": {},
   "source": [
    "1단계: $h_{t-1}$ 과 $x_t$ 를 연결 (concatenate)\n",
    "\n",
    "   $[ h_{t-1}, x_t]$ = [0.1,    0.3,    -0.2,    0.5,    0.2,    -0.5,    0.8]\n",
    "\n",
    "               $h_{t-1}$    $x_t$\n",
    "\n",
    "                        4개              3개\n",
    "\n",
    "                               ↓\n",
    "\n",
    "                           총 7개 벡터\n",
    "\n",
    "2단계: 가중치 행렬 곱하기\n",
    "   $W_f · [h_{t-1}, x_t]$\n",
    "   \n",
    "   $W_f$ 는 크기: (4, 7) 행렬\n",
    "   (왜 (4, 7)? → 은닉 크기 4개, 입력 7개)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "3단계: 편향 더하기\n",
    "   + $b_f$  (크기: 4)\n",
    "   \n",
    "   결과: 4개의 값\n",
    "\n",
    "4단계: Sigmoid 함수 적용\n",
    "   $σ(x)$ = $\\frac{1}{1 + e^{-x}}$ \n",
    "   \n",
    "   이 함수는 모든 값을 0~1 사이로 변환!\n",
    "   \n",
    "   $f_t = [0.3, 0.8, 0.1, 0.9]$\n",
    "   \n",
    "   의미:\n",
    "   - 첫 번째 셀 상태: 30% 유지 (70% 잊음)\n",
    "   - 두 번째 셀 상태: 80% 유지 (20% 잊음)\n",
    "   - 세 번째 셀 상태: 10% 유지 (90% 잊음)\n",
    "   - 네 번째 셀 상태: 90% 유지 (10% 잊음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef2e115",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'headline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[1;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(a)\n\u001b[0;32m----> 9\u001b[0m     a\u001b[38;5;241m.\u001b[39mheadline\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     10\u001b[0m     all_headline\u001b[38;5;241m.\u001b[39mextend( a\u001b[38;5;241m.\u001b[39mheadline\u001b[38;5;241m.\u001b[39mvalues )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'headline'"
     ]
    }
   ],
   "source": [
    "all_headline = []\n",
    "articles = [path for path in article_lists if \"Articles\" in path]\n",
    "# headline 정보만 추출 all_headline에 추가\n",
    "# 전처리 : 소문자로 변경하고 특수문자 제거\n",
    "import string\n",
    "\n",
    "for a in articles:\n",
    "    df = pd.read_csv(a)\n",
    "    a.headline.values\n",
    "    all_headline.extend( a.headline.values )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(article_lists[0])\n",
    "cleaned_sentence = [doc.lower() for doc in df.headline.values if doc not in string.punctuation ]\n",
    "\n",
    "\n",
    "# 모든 문장의 단어를 추출해 고유 번호 지정\n",
    "bow = {}\n",
    "for line in cleaned_sentence:\n",
    "    for w in line.split():\n",
    "        if w not in bow:\n",
    "            bow[w] = len(bow.keys())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7a7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 29, 30, 31]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bow[w] for w in cleaned_sentence[5].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bce2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0dddd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channel Terms of Service accepted\n",
      "Retrieving notices: done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\playdata2\\miniconda3\\envs\\deep\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ipywidgets-8.1.7           |     pyhd8ed1ab_0         112 KB  conda-forge\n",
      "    jupyterlab_widgets-3.0.15  |     pyhd8ed1ab_0         185 KB  conda-forge\n",
      "    widgetsnbextension-4.0.14  |     pyhd8ed1ab_0         868 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ipywidgets         conda-forge/noarch::ipywidgets-8.1.7-pyhd8ed1ab_0 \n",
      "  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-3.0.15-pyhd8ed1ab_0 \n",
      "  widgetsnbextension conda-forge/noarch::widgetsnbextension-4.0.14-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working...\n",
      "widgetsnbextension-4 | 868 KB    |            |   0% \n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    |            |   0% \u001b[A\n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    |            |   0% \u001b[A\u001b[A\n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    | ########## | 100% \u001b[A\n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "widgetsnbextension-4 | 868 KB    | ########## | 100% \n",
      "widgetsnbextension-4 | 868 KB    | ########## | 100% \n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    | ########## | 100% \u001b[A\n",
      "\n",
      "jupyterlab_widgets-3 | 185 KB    | ########## | 100% \u001b[A\n",
      "widgetsnbextension-4 | 868 KB    | ########## | 100% \n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ipywidgets-8.1.7     | 112 KB    | ########## | 100% \u001b[A\u001b[A\n",
      "                                                     \n",
      "\n",
      "\n",
      "                                                     \u001b[A\n",
      "\n",
      "\n",
      "                                                     \u001b[A\u001b[A done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52fed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# kaggle data download\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download('aashita/nyt-comments')\n",
    "\n",
    "# csv파일이 있는 경로 path\n",
    "csv_lists = glob(path+'/*.*')\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    def __init__(self,csv_lists):\n",
    "        all_headlines = []\n",
    "\n",
    "        # 모든 헤드라인의 텍스트를 불러옴\n",
    "        for filename in csv_lists:\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        # headline 중 unknown 값은 제거\n",
    "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "        \n",
    "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        # 모델의 입력으로 사용할 데이터\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) \n",
    "                                        for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])  # 입력 데이터\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)  # 출력 데이터\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d385004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array(2., dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextGeneration(csv_lists)\n",
    "x, y = next(iter(dataset))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a37481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):  # num_embeddings 전체 단어의 개수(어휘사전 크기)\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # 신경망이 이해할수 있도록 벡터로 변경\n",
    "        self.embed = nn.Embedding( num_embeddings=num_embeddings, embedding_dim=16)\n",
    "\n",
    "        # LSTM을 5개층  (배치, 시퀀스, 피처)  16 ~ 512\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=64, num_layers=5,batch_first=True)\n",
    "        # 분류를위한 fc층  squence_length * 64 = 2*64  128\n",
    "        self.fc1 = nn.Linear( 128, num_embeddings)\n",
    "        self.fc2 = nn.Linear( num_embeddings , num_embeddings)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):  # 입력 (batch , sq_len) batch: 32  sq_len : 2\n",
    "        x = self.embed(x)  # 출력 (batch, sq_len, 16)  32 2 16\n",
    "\n",
    "        # lstm 모델 예측값\n",
    "        x, _ =  self.lstm(x) # 출력 (batch ,sq_len, 64)  32 2 64\n",
    "        x = torch.reshape(x, (x.shape[0], -1))  # 출력 (batch, sq_len x 64)  32, 128\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "027c16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2356"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d26444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/117 [00:00<?, ?it/s]/var/folders/1l/rpxsxc3j1b15kw96kqt86gh80000gn/T/ipykernel_12141/2990939988.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long))\n",
      "/var/folders/1l/rpxsxc3j1b15kw96kqt86gh80000gn/T/ipykernel_12141/2990939988.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n",
      "epoch:1 loss : 7.150957107543945: 100%|██████████| 117/117 [00:01<00:00, 61.73it/s] \n",
      "epoch:2 loss : 6.827028274536133: 100%|██████████| 117/117 [00:01<00:00, 72.33it/s]\n",
      "epoch:3 loss : 6.617111682891846: 100%|██████████| 117/117 [00:01<00:00, 71.47it/s] \n",
      "epoch:4 loss : 6.3618693351745605: 100%|██████████| 117/117 [00:01<00:00, 70.95it/s]\n",
      "epoch:5 loss : 6.16735315322876: 100%|██████████| 117/117 [00:01<00:00, 72.33it/s]  \n",
      "epoch:6 loss : 6.07197380065918: 100%|██████████| 117/117 [00:01<00:00, 72.12it/s]  \n",
      "epoch:7 loss : 6.021004676818848: 100%|██████████| 117/117 [00:01<00:00, 72.33it/s] \n",
      "epoch:8 loss : 6.012490272521973: 100%|██████████| 117/117 [00:01<00:00, 68.51it/s]\n",
      "epoch:9 loss : 5.999006271362305: 100%|██████████| 117/117 [00:01<00:00, 60.98it/s] \n",
      "epoch:10 loss : 6.220035552978516: 100%|██████████| 117/117 [00:01<00:00, 71.81it/s] \n",
      "epoch:11 loss : 6.254245281219482: 100%|██████████| 117/117 [00:01<00:00, 72.27it/s] \n",
      "epoch:12 loss : 6.64614200592041: 100%|██████████| 117/117 [00:01<00:00, 72.44it/s]  \n",
      "epoch:13 loss : 6.727346897125244: 100%|██████████| 117/117 [00:01<00:00, 71.15it/s] \n",
      "epoch:14 loss : 6.3969526290893555: 100%|██████████| 117/117 [00:01<00:00, 69.62it/s]\n",
      "epoch:15 loss : 6.351959228515625: 100%|██████████| 117/117 [00:01<00:00, 72.32it/s]\n",
      "epoch:16 loss : 6.316410064697266: 100%|██████████| 117/117 [00:01<00:00, 72.44it/s] \n",
      "epoch:17 loss : 6.168527126312256: 100%|██████████| 117/117 [00:01<00:00, 72.23it/s] \n",
      "epoch:18 loss : 6.252649784088135: 100%|██████████| 117/117 [00:01<00:00, 64.45it/s] \n",
      "epoch:19 loss : 6.0877156257629395: 100%|██████████| 117/117 [00:01<00:00, 72.65it/s]\n",
      "epoch:20 loss : 6.0465989112854: 100%|██████████| 117/117 [00:01<00:00, 70.38it/s]   \n",
      "epoch:21 loss : 5.918197154998779: 100%|██████████| 117/117 [00:01<00:00, 71.94it/s] \n",
      "epoch:22 loss : 5.901612758636475: 100%|██████████| 117/117 [00:01<00:00, 71.19it/s] \n",
      "epoch:23 loss : 5.824944019317627: 100%|██████████| 117/117 [00:01<00:00, 71.14it/s] \n",
      "epoch:24 loss : 5.8774943351745605: 100%|██████████| 117/117 [00:01<00:00, 71.53it/s]\n",
      "epoch:25 loss : 5.811179161071777: 100%|██████████| 117/117 [00:01<00:00, 69.73it/s] \n",
      "epoch:26 loss : 5.83349084854126: 100%|██████████| 117/117 [00:01<00:00, 71.44it/s] \n",
      "epoch:27 loss : 5.779253005981445: 100%|██████████| 117/117 [00:01<00:00, 63.03it/s] \n",
      "epoch:28 loss : 5.652106285095215: 100%|██████████| 117/117 [00:01<00:00, 67.37it/s] \n",
      "epoch:29 loss : 5.591030597686768: 100%|██████████| 117/117 [00:01<00:00, 71.19it/s] \n",
      "epoch:30 loss : 5.584889888763428: 100%|██████████| 117/117 [00:01<00:00, 69.96it/s] \n",
      "epoch:31 loss : 5.610929489135742: 100%|██████████| 117/117 [00:01<00:00, 71.94it/s]\n",
      "epoch:32 loss : 6.002251625061035: 100%|██████████| 117/117 [00:01<00:00, 71.40it/s] \n",
      "epoch:33 loss : 5.573774337768555: 100%|██████████| 117/117 [00:01<00:00, 71.78it/s]\n",
      "epoch:34 loss : 5.964046955108643: 100%|██████████| 117/117 [00:01<00:00, 70.04it/s] \n",
      "epoch:35 loss : 5.621290683746338: 100%|██████████| 117/117 [00:01<00:00, 71.40it/s] \n",
      "epoch:36 loss : 5.632099151611328: 100%|██████████| 117/117 [00:01<00:00, 72.04it/s]\n",
      "epoch:37 loss : 5.529672622680664: 100%|██████████| 117/117 [00:01<00:00, 71.93it/s] \n",
      "epoch:38 loss : 5.6886515617370605: 100%|██████████| 117/117 [00:01<00:00, 71.15it/s]\n",
      "epoch:39 loss : 5.60659646987915: 100%|██████████| 117/117 [00:01<00:00, 71.82it/s] \n",
      "epoch:40 loss : 5.674062252044678: 100%|██████████| 117/117 [00:01<00:00, 71.71it/s] \n",
      "epoch:41 loss : 5.332744598388672: 100%|██████████| 117/117 [00:01<00:00, 70.13it/s]\n",
      "epoch:42 loss : 5.515953063964844: 100%|██████████| 117/117 [00:01<00:00, 71.96it/s] \n",
      "epoch:43 loss : 5.338738918304443: 100%|██████████| 117/117 [00:01<00:00, 69.31it/s] \n",
      "epoch:44 loss : 5.3570027351379395: 100%|██████████| 117/117 [00:01<00:00, 66.91it/s]\n",
      "epoch:45 loss : 5.288354873657227: 100%|██████████| 117/117 [00:01<00:00, 60.27it/s] \n",
      "epoch:46 loss : 5.456360340118408: 100%|██████████| 117/117 [00:01<00:00, 70.42it/s]\n",
      "epoch:47 loss : 5.193431854248047: 100%|██████████| 117/117 [00:01<00:00, 70.73it/s] \n",
      "epoch:48 loss : 5.385330677032471: 100%|██████████| 117/117 [00:01<00:00, 68.98it/s] \n",
      "epoch:49 loss : 5.3708577156066895: 100%|██████████| 117/117 [00:02<00:00, 53.37it/s]\n",
      "epoch:50 loss : 5.649298191070557: 100%|██████████| 117/117 [00:02<00:00, 56.36it/s] \n",
      "epoch:51 loss : 5.397550106048584: 100%|██████████| 117/117 [00:02<00:00, 54.73it/s] \n",
      "epoch:52 loss : 5.4454026222229: 100%|██████████| 117/117 [00:01<00:00, 63.94it/s]  \n",
      "epoch:53 loss : 5.296595096588135: 100%|██████████| 117/117 [00:01<00:00, 60.82it/s] \n",
      "epoch:54 loss : 5.670206546783447: 100%|██████████| 117/117 [00:01<00:00, 64.12it/s] \n",
      "epoch:55 loss : 5.468907833099365: 100%|██████████| 117/117 [00:02<00:00, 52.99it/s] \n",
      "epoch:56 loss : 5.4413557052612305: 100%|██████████| 117/117 [00:02<00:00, 55.36it/s]\n",
      "epoch:57 loss : 5.165318489074707: 100%|██████████| 117/117 [00:01<00:00, 61.78it/s] \n",
      "epoch:58 loss : 5.264206409454346: 100%|██████████| 117/117 [00:01<00:00, 63.67it/s] \n",
      "epoch:59 loss : 4.994464874267578: 100%|██████████| 117/117 [00:01<00:00, 63.33it/s] \n",
      "epoch:60 loss : 5.220137596130371: 100%|██████████| 117/117 [00:01<00:00, 68.65it/s] \n",
      "epoch:61 loss : 4.98421049118042: 100%|██████████| 117/117 [00:02<00:00, 55.81it/s] \n",
      "epoch:62 loss : 5.277293682098389: 100%|██████████| 117/117 [00:01<00:00, 67.37it/s]\n",
      "epoch:63 loss : 4.745081901550293: 100%|██████████| 117/117 [00:01<00:00, 70.69it/s] \n",
      "epoch:64 loss : 5.066920280456543: 100%|██████████| 117/117 [00:01<00:00, 72.56it/s]\n",
      "epoch:65 loss : 5.096071720123291: 100%|██████████| 117/117 [00:01<00:00, 72.46it/s] \n",
      "epoch:66 loss : 5.070318698883057: 100%|██████████| 117/117 [00:01<00:00, 72.44it/s] \n",
      "epoch:67 loss : 4.795474052429199: 100%|██████████| 117/117 [00:01<00:00, 70.91it/s] \n",
      "epoch:68 loss : 4.738616466522217: 100%|██████████| 117/117 [00:01<00:00, 72.90it/s] \n",
      "epoch:69 loss : 4.461215019226074: 100%|██████████| 117/117 [00:01<00:00, 72.56it/s]\n",
      "epoch:70 loss : 4.85911226272583: 100%|██████████| 117/117 [00:01<00:00, 70.38it/s]  \n",
      "epoch:71 loss : 4.433728218078613: 100%|██████████| 117/117 [00:01<00:00, 71.60it/s] \n",
      "epoch:72 loss : 4.953075885772705: 100%|██████████| 117/117 [00:02<00:00, 58.18it/s] \n",
      "epoch:73 loss : 4.740004539489746: 100%|██████████| 117/117 [00:02<00:00, 56.35it/s] \n",
      "epoch:74 loss : 4.790966033935547: 100%|██████████| 117/117 [00:01<00:00, 59.76it/s] \n",
      "epoch:75 loss : 4.642224311828613: 100%|██████████| 117/117 [00:02<00:00, 56.94it/s] \n",
      "epoch:76 loss : 4.878146171569824: 100%|██████████| 117/117 [00:01<00:00, 59.69it/s] \n",
      "epoch:77 loss : 4.8629961013793945: 100%|██████████| 117/117 [00:02<00:00, 57.63it/s]\n",
      "epoch:78 loss : 4.916674613952637: 100%|██████████| 117/117 [00:01<00:00, 66.97it/s] \n",
      "epoch:79 loss : 5.7623796463012695: 100%|██████████| 117/117 [00:01<00:00, 61.58it/s]\n",
      "epoch:80 loss : 4.698174953460693: 100%|██████████| 117/117 [00:01<00:00, 64.77it/s] \n",
      "epoch:81 loss : 4.55076265335083: 100%|██████████| 117/117 [00:01<00:00, 61.88it/s] \n",
      "epoch:82 loss : 4.5198845863342285: 100%|██████████| 117/117 [00:02<00:00, 57.87it/s]\n",
      "epoch:83 loss : 5.130836486816406: 100%|██████████| 117/117 [00:01<00:00, 66.06it/s] \n",
      "epoch:84 loss : 4.600745677947998: 100%|██████████| 117/117 [00:01<00:00, 69.38it/s] \n",
      "epoch:85 loss : 4.355898857116699: 100%|██████████| 117/117 [00:01<00:00, 63.66it/s] \n",
      "epoch:86 loss : 4.775891304016113: 100%|██████████| 117/117 [00:01<00:00, 64.19it/s] \n",
      "epoch:87 loss : 4.336941719055176: 100%|██████████| 117/117 [00:01<00:00, 65.26it/s] \n",
      "epoch:88 loss : 5.090792179107666: 100%|██████████| 117/117 [00:01<00:00, 68.14it/s] \n",
      "epoch:89 loss : 4.480804920196533: 100%|██████████| 117/117 [00:01<00:00, 65.49it/s] \n",
      "epoch:90 loss : 4.633048057556152: 100%|██████████| 117/117 [00:02<00:00, 54.48it/s]\n",
      "epoch:91 loss : 4.480785369873047: 100%|██████████| 117/117 [00:02<00:00, 58.28it/s] \n",
      "epoch:92 loss : 4.618892192840576: 100%|██████████| 117/117 [00:01<00:00, 60.57it/s] \n",
      "epoch:93 loss : 4.703941822052002: 100%|██████████| 117/117 [00:01<00:00, 60.98it/s] \n",
      "epoch:94 loss : 5.0698137283325195: 100%|██████████| 117/117 [00:01<00:00, 65.01it/s]\n",
      "epoch:95 loss : 5.474596977233887: 100%|██████████| 117/117 [00:02<00:00, 56.15it/s] \n",
      "epoch:96 loss : 5.055053234100342: 100%|██████████| 117/117 [00:01<00:00, 63.52it/s]\n",
      "epoch:97 loss : 5.056855201721191: 100%|██████████| 117/117 [00:01<00:00, 64.49it/s] \n",
      "epoch:98 loss : 5.031955718994141: 100%|██████████| 117/117 [00:01<00:00, 61.88it/s]\n",
      "epoch:99 loss : 4.968698501586914: 100%|██████████| 117/117 [00:01<00:00, 71.70it/s]\n",
      "epoch:100 loss : 5.014616966247559: 100%|██████████| 117/117 [00:01<00:00, 60.15it/s] \n",
      "epoch:101 loss : 5.114549160003662: 100%|██████████| 117/117 [00:01<00:00, 59.38it/s] \n",
      "epoch:102 loss : 4.96796989440918: 100%|██████████| 117/117 [00:01<00:00, 64.34it/s]  \n",
      "epoch:103 loss : 4.936049938201904: 100%|██████████| 117/117 [00:01<00:00, 70.61it/s] \n",
      "epoch:104 loss : 5.030563831329346: 100%|██████████| 117/117 [00:01<00:00, 71.71it/s] \n",
      "epoch:105 loss : 4.8869853019714355: 100%|██████████| 117/117 [00:01<00:00, 72.87it/s]\n",
      "epoch:106 loss : 5.072782516479492: 100%|██████████| 117/117 [00:01<00:00, 70.61it/s] \n",
      "epoch:107 loss : 4.876204967498779: 100%|██████████| 117/117 [00:01<00:00, 72.09it/s] \n",
      "epoch:108 loss : 4.968863010406494: 100%|██████████| 117/117 [00:01<00:00, 70.79it/s] \n",
      "epoch:109 loss : 4.883431434631348: 100%|██████████| 117/117 [00:01<00:00, 72.47it/s] \n",
      "epoch:110 loss : 4.8683013916015625: 100%|██████████| 117/117 [00:01<00:00, 68.65it/s]\n",
      "epoch:111 loss : 4.782179832458496: 100%|██████████| 117/117 [00:01<00:00, 71.97it/s] \n",
      "epoch:112 loss : 4.83182430267334: 100%|██████████| 117/117 [00:01<00:00, 71.55it/s]  \n",
      "epoch:113 loss : 4.79719352722168: 100%|██████████| 117/117 [00:02<00:00, 52.55it/s] \n",
      "epoch:114 loss : 4.903353691101074: 100%|██████████| 117/117 [00:01<00:00, 72.77it/s] \n",
      "epoch:115 loss : 5.207317352294922: 100%|██████████| 117/117 [00:01<00:00, 70.33it/s] \n",
      "epoch:116 loss : 4.902796745300293: 100%|██████████| 117/117 [00:01<00:00, 72.77it/s]\n",
      "epoch:117 loss : 4.824122428894043: 100%|██████████| 117/117 [00:01<00:00, 72.45it/s] \n",
      "epoch:118 loss : 4.663780212402344: 100%|██████████| 117/117 [00:01<00:00, 70.51it/s] \n",
      "epoch:119 loss : 4.762016773223877: 100%|██████████| 117/117 [00:01<00:00, 72.23it/s] \n",
      "epoch:120 loss : 4.838540077209473: 100%|██████████| 117/117 [00:01<00:00, 70.76it/s]\n",
      "epoch:121 loss : 4.806019306182861: 100%|██████████| 117/117 [00:01<00:00, 64.06it/s] \n",
      "epoch:122 loss : 4.893209457397461: 100%|██████████| 117/117 [00:01<00:00, 71.53it/s] \n",
      "epoch:123 loss : 4.868248462677002: 100%|██████████| 117/117 [00:01<00:00, 72.00it/s] \n",
      "epoch:124 loss : 4.977756500244141: 100%|██████████| 117/117 [00:01<00:00, 72.39it/s] \n",
      "epoch:125 loss : 4.6989426612854: 100%|██████████| 117/117 [00:01<00:00, 70.74it/s]   \n",
      "epoch:126 loss : 4.619095325469971: 100%|██████████| 117/117 [00:01<00:00, 72.44it/s] \n",
      "epoch:127 loss : 4.622268199920654: 100%|██████████| 117/117 [00:01<00:00, 70.72it/s]\n",
      "epoch:128 loss : 4.620445251464844: 100%|██████████| 117/117 [00:01<00:00, 72.20it/s] \n",
      "epoch:129 loss : 4.597488880157471: 100%|██████████| 117/117 [00:01<00:00, 70.25it/s] \n",
      "epoch:130 loss : 4.588116645812988: 100%|██████████| 117/117 [00:01<00:00, 64.06it/s]\n",
      "epoch:131 loss : 4.610340118408203: 100%|██████████| 117/117 [00:01<00:00, 70.91it/s]\n",
      "epoch:132 loss : 4.6342692375183105: 100%|██████████| 117/117 [00:01<00:00, 72.16it/s]\n",
      "epoch:133 loss : 4.477075576782227: 100%|██████████| 117/117 [00:01<00:00, 70.74it/s] \n",
      "epoch:134 loss : 4.596642971038818: 100%|██████████| 117/117 [00:01<00:00, 72.21it/s] \n",
      "epoch:135 loss : 4.390437126159668: 100%|██████████| 117/117 [00:01<00:00, 70.70it/s]\n",
      "epoch:136 loss : 4.377293586730957: 100%|██████████| 117/117 [00:01<00:00, 72.63it/s] \n",
      "epoch:137 loss : 4.187385082244873: 100%|██████████| 117/117 [00:01<00:00, 70.94it/s]\n",
      "epoch:138 loss : 4.2874956130981445: 100%|██████████| 117/117 [00:01<00:00, 73.09it/s]\n",
      "epoch:139 loss : 4.352000713348389: 100%|██████████| 117/117 [00:01<00:00, 71.09it/s] \n",
      "epoch:140 loss : 4.254592418670654: 100%|██████████| 117/117 [00:01<00:00, 70.08it/s] \n",
      "epoch:141 loss : 4.59705924987793: 100%|██████████| 117/117 [00:01<00:00, 59.64it/s]  \n",
      "epoch:142 loss : 4.487323760986328: 100%|██████████| 117/117 [00:02<00:00, 55.77it/s] \n",
      "epoch:143 loss : 4.333972930908203: 100%|██████████| 117/117 [00:01<00:00, 59.22it/s] \n",
      "epoch:144 loss : 4.342193126678467: 100%|██████████| 117/117 [00:01<00:00, 61.15it/s] \n",
      "epoch:145 loss : 4.081403732299805: 100%|██████████| 117/117 [00:01<00:00, 60.68it/s] \n",
      "epoch:146 loss : 4.149332523345947: 100%|██████████| 117/117 [00:01<00:00, 71.48it/s] \n",
      "epoch:147 loss : 4.233230113983154: 100%|██████████| 117/117 [00:01<00:00, 70.43it/s] \n",
      "epoch:148 loss : 4.0550971031188965: 100%|██████████| 117/117 [00:01<00:00, 69.63it/s]\n",
      "epoch:149 loss : 4.333033561706543: 100%|██████████| 117/117 [00:01<00:00, 72.80it/s] \n",
      "epoch:150 loss : 4.187353610992432: 100%|██████████| 117/117 [00:01<00:00, 71.30it/s] \n",
      "epoch:151 loss : 4.348000526428223: 100%|██████████| 117/117 [00:01<00:00, 71.70it/s]\n",
      "epoch:152 loss : 4.085157871246338: 100%|██████████| 117/117 [00:01<00:00, 70.31it/s]\n",
      "epoch:153 loss : 4.049201488494873: 100%|██████████| 117/117 [00:01<00:00, 70.49it/s] \n",
      "epoch:154 loss : 4.085973262786865: 100%|██████████| 117/117 [00:01<00:00, 68.50it/s]\n",
      "epoch:155 loss : 4.048954963684082: 100%|██████████| 117/117 [00:01<00:00, 62.27it/s] \n",
      "epoch:156 loss : 3.892082452774048: 100%|██████████| 117/117 [00:01<00:00, 66.06it/s] \n",
      "epoch:157 loss : 3.81368088722229: 100%|██████████| 117/117 [00:01<00:00, 70.06it/s] \n",
      "epoch:158 loss : 3.8496246337890625: 100%|██████████| 117/117 [00:01<00:00, 72.51it/s]\n",
      "epoch:159 loss : 3.814150333404541: 100%|██████████| 117/117 [00:01<00:00, 69.22it/s] \n",
      "epoch:160 loss : 3.943422794342041: 100%|██████████| 117/117 [00:01<00:00, 70.68it/s] \n",
      "epoch:161 loss : 3.837594509124756: 100%|██████████| 117/117 [00:01<00:00, 70.25it/s] \n",
      "epoch:162 loss : 3.820770502090454: 100%|██████████| 117/117 [00:01<00:00, 72.90it/s]\n",
      "epoch:163 loss : 3.74452805519104: 100%|██████████| 117/117 [00:01<00:00, 70.99it/s]  \n",
      "epoch:164 loss : 3.681339740753174: 100%|██████████| 117/117 [00:01<00:00, 62.65it/s] \n",
      "epoch:165 loss : 3.7038161754608154: 100%|██████████| 117/117 [00:01<00:00, 73.02it/s]\n",
      "epoch:166 loss : 3.8673105239868164: 100%|██████████| 117/117 [00:01<00:00, 70.18it/s]\n",
      "epoch:167 loss : 3.78432035446167: 100%|██████████| 117/117 [00:01<00:00, 72.95it/s]  \n",
      "epoch:168 loss : 3.541135787963867: 100%|██████████| 117/117 [00:01<00:00, 70.53it/s] \n",
      "epoch:169 loss : 3.5619983673095703: 100%|██████████| 117/117 [00:01<00:00, 69.77it/s]\n",
      "epoch:170 loss : 3.618612766265869: 100%|██████████| 117/117 [00:01<00:00, 69.71it/s] \n",
      "epoch:171 loss : 3.709770917892456: 100%|██████████| 117/117 [00:01<00:00, 65.89it/s] \n",
      "epoch:172 loss : 3.6295011043548584: 100%|██████████| 117/117 [00:01<00:00, 67.65it/s]\n",
      "epoch:173 loss : 3.569993495941162: 100%|██████████| 117/117 [00:01<00:00, 70.51it/s] \n",
      "epoch:174 loss : 3.7881433963775635: 100%|██████████| 117/117 [00:01<00:00, 67.44it/s]\n",
      "epoch:175 loss : 3.4718968868255615: 100%|██████████| 117/117 [00:01<00:00, 70.87it/s]\n",
      "epoch:176 loss : 3.4245266914367676: 100%|██████████| 117/117 [00:02<00:00, 57.42it/s]\n",
      "epoch:177 loss : 3.337329387664795: 100%|██████████| 117/117 [00:01<00:00, 70.61it/s] \n",
      "epoch:178 loss : 3.4198007583618164: 100%|██████████| 117/117 [00:01<00:00, 72.26it/s]\n",
      "epoch:179 loss : 3.34116792678833: 100%|██████████| 117/117 [00:01<00:00, 69.54it/s]  \n",
      "epoch:180 loss : 3.3618996143341064: 100%|██████████| 117/117 [00:01<00:00, 72.62it/s]\n",
      "epoch:181 loss : 3.297011613845825: 100%|██████████| 117/117 [00:01<00:00, 68.04it/s] \n",
      "epoch:182 loss : 3.2518632411956787: 100%|██████████| 117/117 [00:01<00:00, 69.93it/s]\n",
      "epoch:183 loss : 3.2309043407440186: 100%|██████████| 117/117 [00:01<00:00, 72.08it/s]\n",
      "epoch:184 loss : 3.2654731273651123: 100%|██████████| 117/117 [00:01<00:00, 67.23it/s]\n",
      "epoch:185 loss : 3.198507785797119: 100%|██████████| 117/117 [00:01<00:00, 68.69it/s] \n",
      "epoch:186 loss : 3.0566484928131104: 100%|██████████| 117/117 [00:01<00:00, 71.81it/s]\n",
      "epoch:187 loss : 3.104646682739258: 100%|██████████| 117/117 [00:01<00:00, 69.93it/s] \n",
      "epoch:188 loss : 3.2794504165649414: 100%|██████████| 117/117 [00:01<00:00, 71.57it/s]\n",
      "epoch:189 loss : 3.1215827465057373: 100%|██████████| 117/117 [00:01<00:00, 70.91it/s]\n",
      "epoch:190 loss : 3.0097596645355225: 100%|██████████| 117/117 [00:01<00:00, 69.70it/s]\n",
      "epoch:191 loss : 3.0899624824523926: 100%|██████████| 117/117 [00:01<00:00, 71.90it/s]\n",
      "epoch:192 loss : 3.216198205947876: 100%|██████████| 117/117 [00:01<00:00, 70.65it/s] \n",
      "epoch:193 loss : 2.914907455444336: 100%|██████████| 117/117 [00:01<00:00, 69.44it/s] \n",
      "epoch:194 loss : 3.1363604068756104: 100%|██████████| 117/117 [00:01<00:00, 71.78it/s]\n",
      "epoch:195 loss : 2.9411368370056152: 100%|██████████| 117/117 [00:01<00:00, 67.99it/s]\n",
      "epoch:196 loss : 2.856192111968994: 100%|██████████| 117/117 [00:01<00:00, 70.77it/s] \n",
      "epoch:197 loss : 3.0013487339019775: 100%|██████████| 117/117 [00:01<00:00, 71.38it/s]\n",
      "epoch:198 loss : 2.8526670932769775: 100%|██████████| 117/117 [00:01<00:00, 70.62it/s]\n",
      "epoch:199 loss : 2.851402521133423: 100%|██████████| 117/117 [00:01<00:00, 69.30it/s] \n",
      "epoch:200 loss : 2.9793121814727783: 100%|██████████| 117/117 [00:01<00:00, 71.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# 모델....\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = TextGeneration(csv_lists)\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset,batch_size=32)\n",
    "optim = Adam(model.parameters(), lr = 1e-3)\n",
    "for epoch in range(200):\n",
    "    loop = tqdm(loader)\n",
    "    for data, label in loop:\n",
    "        data,label = data.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data, dtype=torch.long))\n",
    "        loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        loop.set_description(f'epoch:{epoch+1} loss : {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 예측\n",
    "# 입력문장을 텐서로 변경 임베딩 벡터 Bow를 이용\n",
    "sample = 'i love'\n",
    "with torch.no_grad():\n",
    "    torch.tensor(\n",
    "        [dataset.Bow[w] for w in sample.split()] , dtype=torch.long\n",
    "    ).to(device).uniqueeze(0)\n",
    "    \n",
    "    output = model(words)\n",
    "    # 출력은 단어의 개수만큼 len(BOW). (bath , len(BOW))\n",
    "    # 확률이 가장 높은 단어 찾기\n",
    "    predicted_index = torch.argmax(output,dim=1).item()\n",
    "    \n",
    "    # 단어사전 BOW에서 index에 해당하는 단어를 찾기\n",
    "    # 역 dict를 만들어서 찾기\n",
    "    revers_bow = {value:key for key , value in dataset.BOW.items()}\n",
    "    predicted_word = revers_bow[predicted_index]\n",
    "    print(predicted_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
